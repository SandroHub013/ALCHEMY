# Main configuration for LLM fine-tuning with Agent Lightning
# Supports Multi-Source Training (Data Mixing) for generalist models

# Base model to load
model:
  name_or_path: "mistralai/Mistral-7B-v0.3"  # v0.3 better for coding and wider context
  # Recommended alternatives:
  #   - "mistralai/Mistral-Nemo-12B" (requires 24GB VRAM or quantization)
  #   - "meta-llama/Llama-2-7b-hf"
  #   - "EleutherAI/gpt-neox-20b"
  trust_remote_code: false
  use_flash_attention_2: false  # Requires flash-attn installed separately
  
# QLoRA and PEFT configuration
peft:
  # Quantization type (4-bit NF4)
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"  # float16 or bfloat16
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
  
  # LoRA configuration
  lora:
    r: 16  # LoRA rank (typically 8-64)
    lora_alpha: 32  # Scaling factor (typically 2x r)
    target_modules:  # Target modules for LoRA (model-specific)
      # For Mistral: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      # For Llama: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      # For GPT-NeoX: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# =============================================================================
# MULTI-SOURCE TRAINING (Data Mixing) - For Generalist Models
# =============================================================================
# This configuration allows mixing multiple datasets with different weights
# to avoid Catastrophic Forgetting and create an all-purpose model.
#
# Recommended recipe for Agentic + Coding model:
#   - 30% Function Calling (Agentic)
#   - 30% Coding
#   - 30% Reasoning/Chat
#   - 10% Identity/Italian

datasets:
  # Enable multi-source training
  multi_source_enabled: true
  
  # Unified output format for all datasets
  # Options: "chatml", "alpaca", "llama2", "mistral"
  output_format: "chatml"
  
  # List of datasets with weights and configurations
  sources:
    # --- AGENTIC / FUNCTION CALLING (30%) ---
    - name: "glaiveai/glaive-function-calling-v2"
      weight: 0.30
      split: "train"
      type: "function_calling"  # Type to apply correct formatter
      # Column mapping (optional, uses default if not specified)
      columns:
        instruction: "chat"  # Column with instruction/conversation
        response: "answer"   # Column with response
      max_samples: 50000  # Limit number of examples (null = all)
    
    # --- CODING (30%) ---
    - name: "nickrosh/Evol-Instruct-Code-80k-v1"
      weight: 0.30
      split: "train"
      type: "coding"
      columns:
        instruction: "instruction"
        response: "output"
      max_samples: 50000
    
    # --- REASONING / CHAT (30%) ---
    - name: "teknium/OpenHermes-2.5"
      weight: 0.30
      split: "train"
      type: "chat"
      columns:
        # OpenHermes uses "conversations" format with messages
        conversations: "conversations"
      max_samples: 50000
    
    # --- ITALIAN / IDENTITY (10%) ---
    - name: "gsarti/clean_mc4_it"
      weight: 0.10
      split: "train"
      type: "language"
      columns:
        text: "text"  # Monolingual dataset, text only
      max_samples: 10000

# =============================================================================
# LEGACY CONFIGURATION (Single Source) - Backward Compatible
# =============================================================================
# If multi_source_enabled is false, use this classic configuration
data:
  dataset_name: "databricks/databricks-dolly-15k"
  dataset_config: null
  text_column: "instruction"
  response_column: "response"
  max_seq_length: 2048
  train_split: "train"
  val_split: null
  val_split_percentage: 0.1
  
# Training
training:
  output_dir: "./checkpoints"
  num_epochs: 3
  per_device_train_batch_size: 2  # Batch size per GPU
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch = per_device * gradient_accumulation * num_gpus
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 0.3
  save_strategy: "epoch"  # "epoch" or "steps"
  save_steps: 500
  eval_strategy: "epoch"  # "epoch" or "steps"
  eval_steps: 500
  logging_steps: 10
  gradient_checkpointing: true  # Enable gradient checkpointing to save memory
  fp16: true  # Mixed precision training
  bf16: false  # Alternative to fp16 (requires Ampere+ GPU)
  dataloader_num_workers: 4
  seed: 42

# Logging
logging:
  use_wandb: false
  wandb_project: "llm-finetuning"
  wandb_entity: null
  log_dir: "./logs"
  use_tensorboard: true

# Hardware
hardware:
  num_gpus: 1  # Number of GPUs to use (0 = CPU, -1 = all available)
  mixed_precision: "16"  # "16", "bf16", or "32"

# =============================================================================
# AGENT LIGHTNING - RL Training for AI Agents
# =============================================================================
# Microsoft Agent Lightning enables training agents with:
# - GRPO: Reinforcement Learning (Group Relative Policy Optimization)
# - APO: Automatic Prompt Optimization
# - SFT: Advanced Supervised Fine-Tuning
#
# GitHub: https://github.com/microsoft/agent-lightning
# Docs: https://microsoft.github.io/agent-lightning/

agent_lightning:
  # Enable Agent Lightning (requires: pip install agentlightning)
  enabled: true
  
  # Training algorithm
  # Options: "sft", "grpo", "apo"
  # - sft: Supervised Fine-Tuning (classic, faster)
  # - grpo: RL with Group Relative Policy Optimization (best for agents)
  # - apo: Automatic Prompt Optimization (optimizes prompts)
  algorithm: "grpo"
  
  # GRPO Configuration (Reinforcement Learning)
  # Used when algorithm = "grpo"
  grpo:
    num_generations: 4        # Generations per prompt (more = more accurate reward)
    temperature: 0.7          # Sampling temperature (0.7-1.0 for diversity)
    top_p: 0.9               # Nucleus sampling
    max_new_tokens: 512      # Maximum generated tokens
    kl_coef: 0.1             # KL divergence coefficient (regularization)
    gamma: 0.99              # Reward discount factor
    clip_range: 0.2          # PPO clip range (training stability)
    
  # APO Configuration (Automatic Prompt Optimization)
  # Used when algorithm = "apo"
  apo:
    num_prompt_candidates: 5   # Prompt candidates to evaluate
    eval_samples: 20           # Samples to evaluate each prompt
    optimize_system_prompt: true  # Also optimize system prompt
  
  # Reward function
  # Options: "coding", "function_calling", "chat", "combined"
  # - combined: Auto-detect task type (recommended)
  reward_function: "combined"
  
  # LightningStore for tracing
  store_path: "./lightning_store"
  enable_tracing: true

# =============================================================================
# LUFFY - Off-Policy Reasoning Learning
# =============================================================================
# LUFFY: "Learning to Reason under Off-Policy Guidance"
# Framework for improving reasoning capabilities using off-policy traces
# 
# Paper: https://arxiv.org/abs/2504.14945
# GitHub: https://github.com/ElliottYan/LUFFY
#
# Supported modes:
# - luffy: Uses traces from external models (e.g., DeepSeek-R1)
# - exgrpo: Learns from model's own experience
# - hybrid: Combination of both

luffy:
  # Enable LUFFY (requires: verl, vllm)
  enabled: true
  
  # Off-policy mode
  # Options: "luffy", "exgrpo", "hybrid"
  # - luffy: Uses reasoning traces from DeepSeek-R1 or other models
  # - exgrpo: Learns from own experience (prioritized replay)
  # - hybrid: Combines external guidance + own experience
  mode: "luffy"
  
  # Off-policy traces source
  # Supported models: "deepseek-r1", "qwen2.5-math", "openr1-math"
  off_policy_source: "deepseek-r1"
  
  # Path to off-policy traces (JSON)
  # Format: [{"prompt": "...", "response": "...", "reward": 0.9}, ...]
  off_policy_traces_path: "./data/reasoning_traces/deepseek_r1_traces.json"
  
  # Weights for on-policy/off-policy mixing
  off_policy_weight: 0.5  # External traces weight
  on_policy_weight: 0.5   # Own generations weight
  
  # Minimum reward threshold to use off-policy traces
  min_off_policy_reward: 0.5
  
  # Generation parameters
  temperature: 0.7
  top_p: 0.95
  num_generations: 4
  max_new_tokens: 2048  # Long for step-by-step reasoning
  
  # RL parameters
  kl_coef: 0.05         # KL penalty (lower = less conservative)
  clip_range: 0.2       # PPO clipping
  gamma: 0.99           # Discount factor
  gae_lambda: 0.95      # GAE lambda
  
  # ExGRPO specific (when mode = "exgrpo" or "hybrid")
  exgrpo:
    experience_buffer_size: 10000   # Experience buffer size
    experience_sample_ratio: 0.3    # Percentage to reuse
    priority_alpha: 0.6             # Prioritized replay alpha
    min_reward_threshold: 0.6       # Minimum threshold to save experience

# =============================================================================
# SEARCH-R1 - Reasoning with Search Integration
# =============================================================================
# Search-R1: RL training for reasoning with integrated search
# Enables the model to search for information during reasoning
#
# GitHub: https://github.com/PeterGriffinJin/Search-R1
# Inspired by DeepSeek-R1: https://arxiv.org/abs/2501.12948
#
# The model learns to:
# - Decide WHEN to search for information
# - Formulate effective QUERIES
# - INTEGRATE results into reasoning

search_r1:
  # Enable Search-R1 (requires: faiss, rank-bm25)
  enabled: true
  
  # Search engine type
  # Options: "vector", "bm25", "hybrid", "web"
  # - vector: Semantic search with embeddings (recommended for knowledge base)
  # - bm25: Keyword-based search (fast, good for exact text)
  # - hybrid: Combination of vector + BM25 (best quality)
  # - web: Integration with web API (requires additional configuration)
  search_engine_type: "hybrid"
  
  # Maximum number of searches per generation
  max_search_calls: 3
  
  # Number of results to include in context
  context_window: 3
  
  # Special tokens for search (used in prompting)
  search_token: "<search>"
  end_search_token: "</search>"
  context_token: "<context>"
  end_context_token: "</context>"
  
  # Generation parameters
  temperature: 0.7
  top_p: 0.95
  max_new_tokens: 2048
  
  # Reasoning strategies
  use_cot: true        # Chain-of-Thought prompting
  use_reflection: true  # Self-reflection after search
  
  # RL parameters for training
  kl_coef: 0.05
  reward_search_bonus: 0.1       # Bonus for effective search use
  reward_correctness_weight: 0.7 # Weight for answer correctness
  reward_reasoning_weight: 0.3   # Weight for reasoning quality
  
  # Search engine configuration
  vector_search:
    embedding_model: "all-MiniLM-L6-v2"
    use_reranker: true
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  
  bm25:
    k1: 1.5  # BM25 parameter
    b: 0.75  # BM25 parameter
  
  hybrid:
    vector_weight: 0.5
    bm25_weight: 0.5
    fusion_method: "rrf"  # Reciprocal Rank Fusion
  
  # Web search (if search_engine_type = "web")
  web_search:
    provider: "serper"  # "serper", "serpapi", "bing"
    api_key_env: "SEARCH_API_KEY"
    max_results: 5

# =============================================================================
# REASONING BENCHMARKS - Reasoning Capability Evaluation
# =============================================================================
# Benchmarks for evaluating mathematical/logical reasoning capabilities
# Used to evaluate LUFFY and Search-R1

reasoning_benchmarks:
  enabled: true
  
  # Evaluation datasets
  datasets:
    - name: "MATH-500"
      path: "hendrycks/competition_math"
      split: "test[:500]"
    - name: "GSM8K"
      path: "gsm8k"
      split: "test"
    - name: "AIME"
      path: "AI-MO/aimo-validation-aime"
      split: "validation"
  
  # Evaluation method
  evaluation:
    use_math_verify: true  # Use Math-Verify for verification
    pass_at_k: [1, 5]      # Pass@k metrics
    temperature: 0.0       # Greedy for eval (deterministic)
    max_new_tokens: 2048
