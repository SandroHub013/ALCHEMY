# Main configuration for LLM fine-tuning with Agent Lightning
# Supports Multi-Source Training (Data Mixing) for generalist models

# Base model to load
model:
  name_or_path: "mistralai/Mistral-7B-v0.3"  # v0.3 better for coding and wider context
  # Recommended alternatives:
  #   - "mistralai/Mistral-Nemo-12B" (requires 24GB VRAM or quantization)
  #   - "meta-llama/Llama-2-7b-hf"
  #   - "EleutherAI/gpt-neox-20b"
  trust_remote_code: false
  use_flash_attention_2: false  # Requires flash-attn installed separately
  
# QLoRA and PEFT configuration
peft:
  # Quantization type (4-bit NF4)
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"  # float16 or bfloat16
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
  
  # LoRA configuration
  lora:
    r: 16  # LoRA rank (typically 8-64)
    lora_alpha: 32  # Scaling factor (typically 2x r)
    target_modules:  # Target modules for LoRA (model-specific)
      # For Mistral: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      # For Llama: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      # For GPT-NeoX: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# =============================================================================
# MULTI-SOURCE TRAINING (Data Mixing) - For Generalist Models
# =============================================================================
# This configuration allows mixing multiple datasets with different weights
# to avoid Catastrophic Forgetting and create an all-purpose model.
#
# Recommended recipe for Agentic + Coding model:
#   - 30% Function Calling (Agentic)
#   - 30% Coding
#   - 30% Reasoning/Chat
#   - 10% Identity/Italian

datasets:
  # Enable multi-source training
  multi_source_enabled: true
  
  # Unified output format for all datasets
  # Options: "chatml", "alpaca", "llama2", "mistral"
  output_format: "chatml"
  
  # List of datasets with weights and configurations
  sources:
    # --- AGENTIC / FUNCTION CALLING (30%) ---
    - name: "glaiveai/glaive-function-calling-v2"
      weight: 0.30
      split: "train"
      type: "function_calling"  # Type to apply correct formatter
      # Column mapping (optional, uses default if not specified)
      columns:
        instruction: "chat"  # Column with instruction/conversation
        response: "answer"   # Column with response
      max_samples: 50000  # Limit number of examples (null = all)
    
    # --- CODING (30%) ---
    - name: "nickrosh/Evol-Instruct-Code-80k-v1"
      weight: 0.30
      split: "train"
      type: "coding"
      columns:
        instruction: "instruction"
        response: "output"
      max_samples: 50000
    
    # --- REASONING / CHAT (30%) ---
    - name: "teknium/OpenHermes-2.5"
      weight: 0.30
      split: "train"
      type: "chat"
      columns:
        # OpenHermes uses "conversations" format with messages
        conversations: "conversations"
      max_samples: 50000
    
    # --- ITALIAN / IDENTITY (10%) ---
    - name: "gsarti/clean_mc4_it"
      weight: 0.10
      split: "train"
      type: "language"
      columns:
        text: "text"  # Monolingual dataset, text only
      max_samples: 10000

# =============================================================================
# LEGACY CONFIGURATION (Single Source) - Backward Compatible
# =============================================================================
# If multi_source_enabled is false, use this classic configuration
data:
  dataset_name: "databricks/databricks-dolly-15k"
  dataset_config: null
  text_column: "instruction"
  response_column: "response"
  max_seq_length: 2048
  train_split: "train"
  val_split: null
  val_split_percentage: 0.1
  
# Training
training:
  output_dir: "./checkpoints"
  num_epochs: 3
  per_device_train_batch_size: 2  # Batch size per GPU
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch = per_device * gradient_accumulation * num_gpus
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 0.3
  save_strategy: "epoch"  # "epoch" or "steps"
  save_steps: 500
  eval_strategy: "epoch"  # "epoch" or "steps"
  eval_steps: 500
  logging_steps: 10
  gradient_checkpointing: true  # Enable gradient checkpointing to save memory
  fp16: true  # Mixed precision training
  bf16: false  # Alternative to fp16 (requires Ampere+ GPU)
  dataloader_num_workers: 4
  seed: 42

# Logging
logging:
  use_wandb: false
  wandb_project: "llm-finetuning"
  wandb_entity: null
  log_dir: "./logs"
  use_tensorboard: true

# Hardware
hardware:
  num_gpus: 1  # Number of GPUs to use (0 = CPU, -1 = all available)
  mixed_precision: "16"  # "16", "bf16", or "32"

# =============================================================================
# AGENT LIGHTNING - RL Training for AI Agents
# =============================================================================
# Microsoft Agent Lightning enables training agents with:
# - GRPO: Reinforcement Learning (Group Relative Policy Optimization)
# - APO: Automatic Prompt Optimization
# - SFT: Advanced Supervised Fine-Tuning
#
# GitHub: https://github.com/microsoft/agent-lightning
# Docs: https://microsoft.github.io/agent-lightning/

agent_lightning:
  # Enable Agent Lightning (requires: pip install agentlightning)
  enabled: true
  
  # Training algorithm
  # Options: "sft", "grpo", "apo"
  # - sft: Supervised Fine-Tuning (classic, faster)
  # - grpo: RL with Group Relative Policy Optimization (best for agents)
  # - apo: Automatic Prompt Optimization (optimizes prompts)
  algorithm: "grpo"
  
  # GRPO Configuration (Reinforcement Learning)
  # Used when algorithm = "grpo"
  grpo:
    num_generations: 4        # Generations per prompt (more = more accurate reward)
    temperature: 0.7          # Sampling temperature (0.7-1.0 for diversity)
    top_p: 0.9               # Nucleus sampling
    max_new_tokens: 512      # Maximum generated tokens
    kl_coef: 0.1             # KL divergence coefficient (regularization)
    gamma: 0.99              # Reward discount factor
    clip_range: 0.2          # PPO clip range (training stability)
    
  # APO Configuration (Automatic Prompt Optimization)
  # Used when algorithm = "apo"
  apo:
    num_prompt_candidates: 5   # Prompt candidates to evaluate
    eval_samples: 20           # Samples to evaluate each prompt
    optimize_system_prompt: true  # Also optimize system prompt
  
  # Reward function
  # Options: "coding", "function_calling", "chat", "combined"
  # - combined: Auto-detect task type (recommended)
  reward_function: "combined"
  
  # LightningStore for tracing
  store_path: "./lightning_store"
  enable_tracing: true

# =============================================================================
# LUFFY - Off-Policy Reasoning Learning
# =============================================================================
# LUFFY: "Learning to Reason under Off-Policy Guidance"
# Framework for improving reasoning capabilities using off-policy traces
# 
# Paper: https://arxiv.org/abs/2504.14945
# GitHub: https://github.com/ElliottYan/LUFFY
#
# Supported modes:
# - luffy: Uses traces from external models (e.g., DeepSeek-R1)
# - exgrpo: Learns from model's own experience
# - hybrid: Combination of both

luffy:
  # Enable LUFFY (requires: verl, vllm)
  enabled: true
  
  # Off-policy mode
  # Options: "luffy", "exgrpo", "hybrid"
  # - luffy: Uses reasoning traces from DeepSeek-R1 or other models
  # - exgrpo: Learns from own experience (prioritized replay)
  # - hybrid: Combines external guidance + own experience
  mode: "luffy"
  
  # Off-policy traces source
  # Supported models: "deepseek-r1", "qwen2.5-math", "openr1-math"
  off_policy_source: "deepseek-r1"
  
  # Path to off-policy traces (JSON)
  # Format: [{"prompt": "...", "response": "...", "reward": 0.9}, ...]
  off_policy_traces_path: "./data/reasoning_traces/deepseek_r1_traces.json"
  
  # Weights for on-policy/off-policy mixing
  off_policy_weight: 0.5  # External traces weight
  on_policy_weight: 0.5   # Own generations weight
  
  # Minimum reward threshold to use off-policy traces
  min_off_policy_reward: 0.5
  
  # Generation parameters
  temperature: 0.7
  top_p: 0.95
  num_generations: 4
  max_new_tokens: 2048  # Long for step-by-step reasoning
  
  # RL parameters
  kl_coef: 0.05         # KL penalty (lower = less conservative)
  clip_range: 0.2       # PPO clipping
  gamma: 0.99           # Discount factor
  gae_lambda: 0.95      # GAE lambda
  
  # ExGRPO specific (when mode = "exgrpo" or "hybrid")
  exgrpo:
    experience_buffer_size: 10000   # Experience buffer size
    experience_sample_ratio: 0.3    # Percentage to reuse
    priority_alpha: 0.6             # Prioritized replay alpha
    min_reward_threshold: 0.6       # Minimum threshold to save experience

# =============================================================================
# SEARCH-R1 - Reasoning with Search Integration
# =============================================================================
# Search-R1: RL training for reasoning with integrated search
# Enables the model to search for information during reasoning
#
# GitHub: https://github.com/PeterGriffinJin/Search-R1
# Inspired by DeepSeek-R1: https://arxiv.org/abs/2501.12948
#
# The model learns to:
# - Decide WHEN to search for information
# - Formulate effective QUERIES
# - INTEGRATE results into reasoning

search_r1:
  # Enable Search-R1 (requires: faiss, rank-bm25)
  enabled: true
  
  # Search engine type
  # Options: "vector", "bm25", "hybrid", "web"
  # - vector: Semantic search with embeddings (recommended for knowledge base)
  # - bm25: Keyword-based search (fast, good for exact text)
  # - hybrid: Combination of vector + BM25 (best quality)
  # - web: Integration with web API (requires additional configuration)
  search_engine_type: "hybrid"
  
  # Maximum number of searches per generation
  max_search_calls: 3
  
  # Number of results to include in context
  context_window: 3
  
  # Special tokens for search (used in prompting)
  search_token: "<search>"
  end_search_token: "</search>"
  context_token: "<context>"
  end_context_token: "</context>"
  
  # Generation parameters
  temperature: 0.7
  top_p: 0.95
  max_new_tokens: 2048
  
  # Reasoning strategies
  use_cot: true        # Chain-of-Thought prompting
  use_reflection: true  # Self-reflection after search
  
  # RL parameters for training
  kl_coef: 0.05
  reward_search_bonus: 0.1       # Bonus for effective search use
  reward_correctness_weight: 0.7 # Weight for answer correctness
  reward_reasoning_weight: 0.3   # Weight for reasoning quality
  
  # Search engine configuration
  vector_search:
    embedding_model: "all-MiniLM-L6-v2"
    use_reranker: true
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  
  bm25:
    k1: 1.5  # BM25 parameter
    b: 0.75  # BM25 parameter
  
  hybrid:
    vector_weight: 0.5
    bm25_weight: 0.5
    fusion_method: "rrf"  # Reciprocal Rank Fusion
  
  # Web search (if search_engine_type = "web")
  web_search:
    provider: "serper"  # "serper", "serpapi", "bing"
    api_key_env: "SEARCH_API_KEY"
    max_results: 5

# =============================================================================
# REASONING BENCHMARKS - Reasoning Capability Evaluation
# =============================================================================
# Benchmarks for evaluating mathematical/logical reasoning capabilities
# Used to evaluate LUFFY and Search-R1

reasoning_benchmarks:
  enabled: true
  
  # Evaluation datasets
  datasets:
    - name: "MATH-500"
      path: "hendrycks/competition_math"
      split: "test[:500]"
    - name: "GSM8K"
      path: "gsm8k"
      split: "test"
    - name: "AIME"
      path: "AI-MO/aimo-validation-aime"
      split: "validation"
  
  # Evaluation method
  evaluation:
    use_math_verify: true  # Use Math-Verify for verification
    pass_at_k: [1, 5]      # Pass@k metrics
    temperature: 0.0       # Greedy for eval (deterministic)
    max_new_tokens: 2048

# =============================================================================
# META-AGENT - Dynamic Agent Generation (Inspired by PocketFlow)
# =============================================================================
# Meta-Agent: Agents that generate other agents, SOPs, and reward functions
# dynamically based on task requirements.
#
# Inspiration: https://github.com/The-Pocket/PocketFlow
# Concept: "Agents building agents" for autonomous scaling

meta_agent:
  # Enable meta-agent capabilities
  enabled: true
  
  # Use LLM for advanced generation (requires loaded model)
  use_llm_generation: false
  
  # Default agent templates
  default_templates:
    - coding
    - reasoning
    - function_calling
    - rag
    - chat
  
  # SOP generation settings
  sop_generation:
    default_num_steps: 5
    auto_trigger_detection: true
  
  # Blueprint customization
  blueprints:
    default_temperature: 0.7
    default_max_tokens: 512
    default_lora_r: 16
    default_lora_alpha: 32

# =============================================================================
# ADAPTIVE TRAINER - Dynamic Optimization (Inspired by AgentFlow)
# =============================================================================
# Adaptive Trainer: Real-time optimization during training
# Automatically adjusts hyperparameters based on training dynamics
#
# Inspiration: https://github.com/lupantech/AgentFlow
# Concept: "In-the-flow" optimization for agentic systems

adaptive_trainer:
  # Enable adaptive training
  enabled: true
  
  # Analysis windows
  short_window: 10        # Steps for trend detection
  long_window: 50         # Historical context
  
  # Thresholds
  plateau_threshold: 0.001    # Min improvement to not be plateau
  divergence_threshold: 0.5   # Max loss increase before divergence
  improvement_threshold: 0.01 # Min improvement for "improving" state
  
  # Adaptation rates
  lr_reduction_factor: 0.5    # LR reduction multiplier
  lr_increase_factor: 1.2     # LR increase multiplier
  temperature_delta: 0.1      # Temperature adjustment step
  
  # Bounds
  min_lr: 1.0e-7
  max_lr: 1.0e-2
  min_temperature: 0.1
  max_temperature: 1.5
  
  # Curriculum learning
  curriculum:
    enabled: true
    initial_difficulty: 0.3     # Start with easier examples (0-1)
    max_difficulty: 1.0         # Maximum difficulty
    difficulty_increase_rate: 0.1  # How fast to increase
  
  # Early stopping
  patience: 10                  # Steps without improvement before action
  early_stop_patience: 30       # Steps before early stopping
  
  # Logging
  log_every_n_steps: 10
  save_adaptation_history: true
  history_path: "./logs/adaptive_history.json"

# =============================================================================
# SWARM TRAINER - Multi-Agent Orchestration (Inspired by claude-flow)
# =============================================================================
# Swarm Trainer: Parallel multi-agent exploration for training
# Uses swarm intelligence to explore solution space efficiently
#
# Inspiration: https://github.com/ruvnet/claude-flow
# Concept: "Swarm intelligence" for AI agent coordination

swarm_trainer:
  # Enable swarm training
  enabled: true
  
  # Swarm composition
  num_agents: 4                  # Total parallel agents
  num_explorers: 2               # High-temperature explorers
  num_exploiters: 2              # Low-temperature exploiters
  
  # Temperature settings
  explorer_temperature: 0.9      # High temp for diverse exploration
  exploiter_temperature: 0.3     # Low temp for focused exploitation
  
  # Aggregation
  top_k_trajectories: 4          # Best trajectories to keep
  aggregation_method: "best"     # "best", "weighted", "ensemble"
  
  # Coordination
  sync_every_n_steps: 10         # Sync frequency between agents
  communication_enabled: true    # Allow inter-agent communication
  
  # Diversity
  diversity_bonus: 0.1           # Bonus for diverse solutions
  min_diversity_threshold: 0.3   # Minimum diversity required
  
  # Resource management
  max_workers: 4                 # Thread pool size
  timeout_seconds: 60            # Timeout per agent task
  
  # Generation
  max_new_tokens: 512
  
  # Logging
  log_agent_outputs: true
  save_all_trajectories: false
  trajectory_path: "./logs/swarm_trajectories.json"

# =============================================================================
# UNSLOTH - High-Performance Fine-tuning (2x faster, 70% less VRAM)
# =============================================================================
# Unsloth provides significant training optimizations:
# - 2x faster training speed
# - 70% less VRAM usage
# - 10-13x longer context lengths
# - Native RL support (GRPO, DPO, etc.)
#
# Reference: https://github.com/unslothai/unsloth
# Documentation: https://docs.unsloth.ai/
#
# Installation: pip install unsloth

unsloth:
  # Enable Unsloth optimizations (requires: pip install unsloth)
  enabled: true
  
  # Precision mode: "4bit", "8bit", "16bit", "full"
  # - 4bit: QLoRA with NF4 quantization (recommended, lowest VRAM)
  # - 8bit: 8-bit quantization
  # - 16bit: 16-bit LoRA (new in Unsloth)
  # - full: Full precision fine-tuning (requires more VRAM)
  precision: "4bit"
  
  # Maximum sequence length (Unsloth supports very long contexts)
  # With 8GB VRAM: up to ~3000 tokens
  # With 16GB VRAM: up to ~40000 tokens
  # With 24GB VRAM: up to ~80000 tokens
  max_seq_length: 2048
  
  # LoRA configuration (Unsloth's LoRA uses 30% less VRAM)
  lora:
    r: 16                # LoRA rank (8-64 typical)
    lora_alpha: 16       # Scaling factor
    lora_dropout: 0.0    # 0 is optimized in Unsloth
    target_modules:      # Modules to apply LoRA
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    bias: "none"         # "none" is optimized
    use_rslora: false    # Rank-stabilized LoRA
  
  # Gradient checkpointing
  # Options: true, false, "unsloth"
  # "unsloth" saves 30% more VRAM than standard checkpointing
  use_gradient_checkpointing: "unsloth"
  
  # Pre-quantized model aliases (4x faster download)
  # Use these aliases instead of full model names:
  #   "mistral-7b" -> unsloth/mistral-7b-v0.3-bnb-4bit
  #   "llama-3.1-8b" -> unsloth/Meta-Llama-3.1-8B-bnb-4bit
  #   "qwen-2.5-7b" -> unsloth/Qwen2.5-7B-bnb-4bit
  #   "deepseek-r1-8b" -> unsloth/DeepSeek-R1-Distill-Qwen-7B-bnb-4bit
  use_prequantized: true
  
  # RL training with Unsloth
  rl:
    # Algorithm: "sft", "grpo", "dpo", "orpo", "kto", "simpo"
    algorithm: "grpo"
    
    # GRPO-specific settings
    grpo:
      num_generations: 4       # Generations per prompt
      temperature: 0.7         # Sampling temperature
      top_p: 0.95             # Nucleus sampling
      max_new_tokens: 512     # Max generated tokens
      kl_coef: 0.1            # KL divergence coefficient
    
    # DPO-specific settings
    dpo:
      beta: 0.1               # DPO beta parameter
    
    # Reward function: "coding", "reasoning", "correctness", "combined"
    reward_function: "combined"
  
  # Training settings (optimized for Unsloth)
  training:
    num_epochs: 3
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 2.0e-4
    warmup_steps: 10
    optim: "adamw_8bit"       # 8-bit AdamW (memory efficient)
    weight_decay: 0.01
    logging_steps: 1
    save_steps: 100
    fp16: false
    bf16: true                # Recommended if GPU supports it
    seed: 3407
  
  # Model saving options
  save:
    # Method: "lora", "merged_16bit", "merged_4bit", "gguf"
    method: "lora"
    # GGUF quantization (if method = "gguf")
    gguf_quantization: "q4_k_m"  # Options: q4_k_m, q5_k_m, q8_0, etc.
    # Push to HuggingFace Hub
    push_to_hub: false
    hub_model_id: null