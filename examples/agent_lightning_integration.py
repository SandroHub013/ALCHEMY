"""
Advanced Microsoft Agent Lightning integration example.

This example shows how to use Agent Lightning for:
1. RL Training with GRPO (Group Relative Policy Optimization)
2. Custom reward functions
3. Span tracing for debugging
4. Generation with evaluation

For simple usage, use directly:
    python main_agent_lightning.py --config config/config.yaml

GitHub Agent Lightning: https://github.com/microsoft/agent-lightning
"""

from typing import Dict, Any, Optional
import yaml
import torch

from src.models import ModelLoader
from src.data import create_data_module
from src.agent import (
    AgentLightningTrainer,
    AgentLightningConfig,
    TrainingAlgorithm,
    RewardFunction,
    check_agent_lightning_available,
)


def custom_reward_function(
    prompt: str,
    generation: str,
    reference: Optional[str] = None,
) -> float:
    """
    Example of a custom reward function.
    
    You can create your own reward function for specific tasks.
    Must return a float between -1.0 and 1.0.
    
    Args:
        prompt: The original prompt
        generation: The response generated by the model
        reference: Reference response (optional)
        
    Returns:
        Reward score
    """
    reward = 0.0
    
    # Example: reward responses that include Python code
    if "def " in generation or "class " in generation:
        reward += 0.3
    
    # Example: reward responses with good explanations
    explanation_keywords = ["because", "therefore", "this means", "in other words"]
    if any(kw in generation.lower() for kw in explanation_keywords):
        reward += 0.2
    
    # Example: penalize responses that are too short
    if len(generation) < 50:
        reward -= 0.3
    
    # Example: penalize repetitive responses
    words = generation.split()
    if len(words) > 5:
        unique_ratio = len(set(words)) / len(words)
        if unique_ratio < 0.5:
            reward -= 0.3  # Very repetitive
    
    return max(-1.0, min(1.0, reward))


def main():
    """Example of advanced usage with Agent Lightning."""
    
    # Check availability
    if not check_agent_lightning_available():
        print("âŒ Agent Lightning not installed!")
        print("   Install with: pip install agentlightning")
        return
    
    print("âœ… Agent Lightning available")
    
    # Load configuration
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    # Load model
    print("ðŸ”„ Loading model...")
    model_config = config["model"]
    peft_config = config["peft"]
    
    quantization_config = peft_config.get("quantization", {})
    if isinstance(quantization_config.get("bnb_4bit_compute_dtype"), str):
        dtype_str = quantization_config["bnb_4bit_compute_dtype"]
        quantization_config["bnb_4bit_compute_dtype"] = getattr(torch, dtype_str, torch.float16)
    
    model_loader = ModelLoader(
        model_name_or_path=model_config["name_or_path"],
        quantization_config=quantization_config,
        lora_config=peft_config.get("lora", {}),
    )
    
    model, tokenizer = model_loader.get_model_and_tokenizer()
    print(f"âœ… Model loaded: {model_config['name_or_path']}")
    
    # Create Agent Lightning configuration
    agl_config = AgentLightningConfig(
        algorithm=TrainingAlgorithm.GRPO,  # Use RL!
        grpo_config={
            "num_generations": 4,
            "temperature": 0.7,
            "top_p": 0.9,
            "max_new_tokens": 512,
            "kl_coef": 0.1,
        },
        enable_tracing=True,
        reward_function="combined",  # Or use custom_reward_function
    )
    
    # Create trainer with custom reward function
    trainer = AgentLightningTrainer(
        model=model,
        tokenizer=tokenizer,
        config=agl_config,
        reward_fn=custom_reward_function,  # Use our custom function!
    )
    
    print("âœ… Trainer created with custom reward function")
    
    # Prepare dataset
    print("ðŸ”„ Preparing dataset...")
    data_module = create_data_module(tokenizer=tokenizer, config=config)
    data_module.setup("fit")
    
    train_dataset = data_module.train_dataset
    eval_dataset = data_module.val_dataset
    
    print(f"âœ… Dataset: {len(train_dataset)} training samples")
    
    # Training
    print("\n" + "=" * 60)
    print("ðŸš€ STARTING RL TRAINING WITH GRPO")
    print("=" * 60)
    
    results = trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        num_epochs=1,  # Quick example
        batch_size=2,
        learning_rate=2e-5,
        output_dir="./checkpoints/agent_lightning_example",
    )
    
    print("\nâœ… Training completed!")
    
    # Test generation and reward evaluation
    print("\n" + "=" * 60)
    print("ðŸ§ª GENERATION AND REWARD TEST")
    print("=" * 60)
    
    test_prompts = [
        "Write a Python function to reverse a string.",
        "Explain what neural networks are.",
        "Call the weather API to get temperature in Rome.",
    ]
    
    for prompt in test_prompts:
        print(f"\nðŸ“ Prompt: {prompt}")
        
        # Generate response
        response = trainer.generate(
            prompt=prompt,
            max_new_tokens=150,
            temperature=0.7,
        )
        print(f"ðŸ¤– Response: {response[:200]}...")
        
        # Evaluate reward
        reward = trainer.evaluate_reward(prompt, response)
        print(f"â­ Reward: {reward:.3f}")
        
        # Compare with built-in reward function
        builtin_reward = RewardFunction.combined_reward(prompt, response)
        print(f"ðŸ“Š Reward (built-in): {builtin_reward:.3f}")
    
    print("\nðŸŽ‰ Example completed!")


if __name__ == "__main__":
    main()
