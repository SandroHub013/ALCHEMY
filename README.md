<p align="center">
  <img src="assets/banner.svg" alt="LLM Fine-tuning Agent Lightning" width="100%">
</p>

<h1 align="center">ğŸ§  LLM Fine-tuning con Agent Lightning + LUFFY + Search-R1</h1>

<p align="center">
  <strong>Un framework Python per addestrare modelli linguistici localmente, con Reinforcement Learning avanzato, ragionamento off-policy e ricerca integrata</strong>
</p>

<p align="center">
  <a href="#-la-storia-dietro-il-progetto">La Storia</a> â€¢
  <a href="#-features-principali">Features</a> â€¢
  <a href="#-luffy-off-policy-reasoning">LUFFY</a> â€¢
  <a href="#-search-r1-reasoning-with-search">Search-R1</a> â€¢
  <a href="#-architettura">Architettura</a> â€¢
  <a href="DIAGRAMS.md">ğŸ“Š Diagrammi</a> â€¢
  <a href="#-citazioni-e-riferimenti">Citazioni</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776ab?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Lightning-2.0+-792ee5?style=for-the-badge&logo=lightning&logoColor=white" alt="Lightning">
  <img src="https://img.shields.io/badge/LUFFY-NeurIPS%202025-ff6b6b?style=for-the-badge" alt="LUFFY">
  <img src="https://img.shields.io/badge/DeepSeek--R1-Reasoning-00d4aa?style=for-the-badge" alt="DeepSeek-R1">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License">
</p>

---

## ğŸ¯ La Storia Dietro il Progetto

> *"Come posso far funzionare un modello da 7 miliardi di parametri sulla mia GPU da gaming?"*

Questa domanda, apparentemente semplice, Ã¨ stata il punto di partenza di questo progetto.

Nel 2024, i Large Language Models hanno rivoluzionato il modo in cui interagiamo con le macchine. Ma c'era un problema: allenarli richiedeva cluster di GPU che costano milioni di euro. I modelli open-source esistevano, ma personalizzarli per task specifici sembrava un privilegio riservato ai grandi laboratori di ricerca.

**Questo progetto nasce per cambiare le regole del gioco.**

Ho combinato le tecniche piÃ¹ avanzate della ricerca recente â€” **QLoRA** per la quantizzazione, **PEFT** per l'efficienza parametrica, e **Agent Lightning** di Microsoft per il Reinforcement Learning â€” in un framework unificato che:

- âœ… Funziona su una singola GPU consumer (16GB VRAM)
- âœ… Supporta il training di agenti AI con capacitÃ  di ragionamento
- âœ… Include un sistema RAG completo per la memoria a lungo termine
- âœ… Implementa procedure operative standard (SOP) per comportamenti strutturati

Il risultato? **Un modello che puÃ² essere specializzato per coding, function calling, o qualsiasi altro task â€” sul tuo computer, con i tuoi dati.**

---

## âœ¨ Features Principali

### ğŸ”¬ Training Efficiente

| Feature | Descrizione | Impatto |
|---------|-------------|---------|
| **QLoRA 4-bit** | Quantizzazione NF4 con bitsandbytes | -75% VRAM usage |
| **PEFT/LoRA** | Solo ~1% parametri trainable | Training 50x piÃ¹ veloce |
| **Gradient Checkpointing** | Trade-off memoria/velocitÃ  | Modelli 2x piÃ¹ grandi |
| **Multi-Source Training** | Data mixing per modelli generalisti | No Catastrophic Forgetting |

### ğŸ¤– Agent Lightning Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ALGORITMI DISPONIBILI                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SFT â”€â”€â”€â”€â”€â”€â–º Supervised Fine-Tuning classico                â”‚
â”‚              â€¢ Training iniziale                             â”‚
â”‚              â€¢ Dataset etichettati                           â”‚
â”‚                                                              â”‚
â”‚  GRPO â”€â”€â”€â”€â”€â–º Group Relative Policy Optimization             â”‚
â”‚              â€¢ Reinforcement Learning                        â”‚
â”‚              â€¢ Miglioramento comportamento agente            â”‚
â”‚              â€¢ Reward functions personalizzate               â”‚
â”‚                                                              â”‚
â”‚  APO â”€â”€â”€â”€â”€â”€â–º Automatic Prompt Optimization                  â”‚
â”‚              â€¢ Ottimizza system prompt                       â”‚
â”‚              â€¢ Self-improvement del modello                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¦Š LUFFY - Off-Policy Reasoning

**[LUFFY](https://github.com/ElliottYan/LUFFY)** (Learning to Reason under Off-Policy Guidance) Ã¨ un framework per migliorare le capacitÃ  di ragionamento usando tracce off-policy da modelli avanzati come DeepSeek-R1.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LUFFY TRAINING FLOW                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ DeepSeek-R1 â”‚    â”‚ On-Policy   â”‚    â”‚   ExGRPO    â”‚     â”‚
â”‚  â”‚   Traces    â”‚ +  â”‚ Generations â”‚ +  â”‚ Experience  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                â”‚
â”‚                     Off-Policy Mixer                        â”‚
â”‚                            â”‚                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                    â”‚  GRPO + KL    â”‚                       â”‚
â”‚                    â”‚  Policy Loss  â”‚                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                            â”‚                                â”‚
â”‚                    Improved Reasoning                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Risultati benchmark (LUFFY su Qwen2.5-Math-7B):**

| Model | AIME 2024 | AIME 2025 | MATH-500 | Olympiad | Avg |
|-------|-----------|-----------|----------|----------|-----|
| Baseline | 11.5 | 4.9 | 43.6 | 15.6 | 19.0 |
| **LUFFY** | **29.4** | **23.1** | **87.6** | **57.2** | **50.1** |

### ğŸ” Search-R1 - Reasoning with Search

**[Search-R1](https://github.com/PeterGriffinJin/Search-R1)** permette al modello di cercare informazioni durante il ragionamento, integrando retrieval e reasoning in modo fluido.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SEARCH-R1 REASONING FLOW                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Question: "What is the capital with highest population?"  â”‚
â”‚                            â”‚                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                    â”‚  Think Step 1 â”‚                       â”‚
â”‚                    â”‚  "I need to   â”‚                       â”‚
â”‚                    â”‚   search..."  â”‚                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                            â”‚                                â”‚
â”‚                   <search>capital population</search>       â”‚
â”‚                            â”‚                                â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚               â”‚    ğŸ” Search Engine     â”‚                  â”‚
â”‚               â”‚   Vector + BM25 Hybrid  â”‚                  â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                            â”‚                                â”‚
â”‚                   <context>results...</context>             â”‚
â”‚                            â”‚                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                    â”‚  Think Step 2 â”‚                       â”‚
â”‚                    â”‚  "Based on    â”‚                       â”‚
â”‚                    â”‚   results..." â”‚                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                            â”‚                                â”‚
â”‚                     Final Answer                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§  Sistema di Memoria

```python
# RAG - Retrieval Augmented Generation
from src.memory import VectorStore, create_vector_store

store = create_vector_store(use_reranker=True)
store.add_documents(["La tua knowledge base..."])
results = store.query("Cos'Ã¨ il machine learning?", n_results=3)

# SOP - Standard Operating Procedures
from src.memory import SOPManager, get_system_prompt_with_sop

manager = SOPManager(sop_directory="./data/sops")
prompt = get_system_prompt_with_sop("Aiutami a debuggare questo codice", manager)
```

### ğŸ“Š Smart Chunking

Ispirato a [osgrep](https://github.com/Ryandonofrio3/osgrep), il sistema di chunking usa **tree-sitter** per preservare i confini semantici del codice:

```
âŒ Chunking tradizionale:          âœ… Smart Chunking:
                                    
"def calculate_tax(income,          "# Function: calculate_tax
  rate):                             def calculate_tax(income, rate):
    '''Calcola le tasse'''   â”€â”€â–º       '''Calcola le tasse'''
    tax = income * rate                tax = income * rate
    return tax"                        return tax"
    
(Taglio arbitrario)                 (Preserva la funzione intera)
```

---

## ğŸ—ï¸ Architettura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ARCHITETTURA DEL SISTEMA                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   CONFIG (YAML)   â”‚
                              â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                              â”‚  â€¢ model         â”‚
                              â”‚  â€¢ datasets      â”‚
                              â”‚  â€¢ training      â”‚
                              â”‚  â€¢ agent_light.  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                           â”‚                           â”‚
           â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MODEL LOADER     â”‚   â”‚    DATA MODULE      â”‚   â”‚   MEMORY SYSTEM     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                     â”‚   â”‚                     â”‚   â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  HuggingFace â”‚   â”‚   â”‚  â”‚Multi-Source â”‚   â”‚   â”‚  â”‚ VectorStore â”‚   â”‚
â”‚  â”‚   Model      â”‚   â”‚   â”‚  â”‚  Dataset    â”‚   â”‚   â”‚  â”‚ (ChromaDB)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚          â”‚   â”‚         â”‚          â”‚   â”‚         â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   QLoRA     â”‚   â”‚   â”‚  â”‚  Formatter  â”‚   â”‚   â”‚  â”‚  Reranker   â”‚   â”‚
â”‚  â”‚   4-bit     â”‚   â”‚   â”‚  â”‚  (ChatML)   â”‚   â”‚   â”‚  â”‚ CrossEncoderâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚          â”‚   â”‚         â”‚          â”‚   â”‚         â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    PEFT     â”‚   â”‚   â”‚  â”‚ DataLoader  â”‚   â”‚   â”‚  â”‚    SOP      â”‚   â”‚
â”‚  â”‚    LoRA     â”‚   â”‚   â”‚  â”‚             â”‚   â”‚   â”‚  â”‚  Manager    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚   â”‚                     â”‚   â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚                         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        TRAINING AGENT          â”‚
                    â”‚       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
                    â”‚                                â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚  PyTorch Lightning       â”‚ â”‚
                    â”‚  â”‚  LightningModule         â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚               â”‚               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚  Agent Lightning         â”‚ â”‚
                    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”    â”‚ â”‚
                    â”‚  â”‚  â”‚ SFT â”‚GRPO â”‚ APO â”‚    â”‚ â”‚
                    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚               â”‚               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚   Reward Functions       â”‚ â”‚
                    â”‚  â”‚  â€¢ coding_reward         â”‚ â”‚
                    â”‚  â”‚  â€¢ function_calling      â”‚ â”‚
                    â”‚  â”‚  â€¢ chat_reward           â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      CHECKPOINT       â”‚
                         â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
                         â”‚   LoRA Adapter +     â”‚
                         â”‚   Agent Config       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Come Funziona

### 1ï¸âƒ£ Il Problema della Memoria

Un modello come Mistral 7B richiede ~28GB di VRAM in float32. La mia GPU ha 16GB. Come risolverlo?

**QLoRA** (Quantized Low-Rank Adaptation) combina due tecniche:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUANTIZZAZIONE NF4                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚   float32 (32 bit) â”€â”€â–º NF4 (4 bit) = 8x meno memoria!         â”‚
â”‚                                                                â”‚
â”‚   Come funziona:                                               â”‚
â”‚   1. I pesi sono mappati su 16 valori predefiniti (4 bit)     â”‚
â”‚   2. Distribuzione "Normal Float" ottimizzata per LLM          â”‚
â”‚   3. Double quantization per i parametri di scaling            â”‚
â”‚                                                                â”‚
â”‚   Risultato: 7B parametri â†’ ~4GB invece di ~28GB              â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LoRA                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚   Invece di aggiornare TUTTI i pesi:                          â”‚
â”‚                                                                â”‚
â”‚   W_new = W_old + Î”W                                          â”‚
â”‚                                                                â”‚
â”‚   Decomponimo Î”W in due matrici piccole:                      â”‚
â”‚                                                                â”‚
â”‚   Î”W = A Ã— B    dove A Ã¨ (d Ã— r) e B Ã¨ (r Ã— d)               â”‚
â”‚                                                                â”‚
â”‚   Se d = 4096 e r = 16:                                       â”‚
â”‚   â€¢ Prima: 4096 Ã— 4096 = 16.7M parametri                      â”‚
â”‚   â€¢ Dopo:  4096 Ã— 16 Ã— 2 = 131K parametri (~127x meno!)       â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2ï¸âƒ£ Reinforcement Learning con GRPO

GRPO (Group Relative Policy Optimization) Ã¨ l'algoritmo RL usato da Agent Lightning. Ecco come funziona:

```
                         GRPO TRAINING LOOP
                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Per ogni prompt nel dataset:                              â”‚
  â”‚                                                           â”‚
  â”‚  1. Genera K risposte diverse (temperature > 0)          â”‚
  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
  â”‚     â”‚ Prompt  â”‚â”€â”€â–º [Risposta 1] [Risposta 2] [Risposta 3]â”‚
  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
  â”‚                                                           â”‚
  â”‚  2. Calcola reward per ogni risposta                     â”‚
  â”‚     R(1) = 0.85  R(2) = 0.42  R(3) = 0.91               â”‚
  â”‚                                                           â”‚
  â”‚  3. Normalizza rewards relativamente                      â”‚
  â”‚     Advantage(i) = R(i) - mean(R)                        â”‚
  â”‚                                                           â”‚
  â”‚  4. Aggiorna policy per favorire risposte migliori       â”‚
  â”‚     Loss = -log(Ï€(risposta|prompt)) Ã— Advantage          â”‚
  â”‚                                                           â”‚
  â”‚  5. Aggiungi KL penalty per stabilitÃ                     â”‚
  â”‚     Total_Loss = Policy_Loss + Î² Ã— KL(Ï€, Ï€_ref)          â”‚
  â”‚                                                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3ï¸âƒ£ Sistema RAG con Reranking

Il retrieval ha due fasi per massimizzare la precisione:

```
Query: "Come implemento il pattern Observer in Python?"
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   FASE 1: Bi-Encoder    â”‚
              â”‚   (Recall veloce)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        Embedding query â”€â”€â”¼â”€â”€ Cosine similarity
                          â”‚   con documenti indicizzati
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Top-K candidati (~20)  â”‚
              â”‚  Score: similaritÃ       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  FASE 2: Cross-Encoder  â”‚
              â”‚  (Precision accurata)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
     Valuta ogni coppia â”€â”€â”¼â”€â”€ (query, documento)
     direttamente         â”‚   con attention
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Top-N finali (~3)     â”‚
              â”‚   Score: rilevanza      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Il Codice Spiegato

### ModelLoader: Caricamento Efficiente

```python
# src/models/model_loader.py

class ModelLoader:
    """
    Il cuore del caricamento modelli.
    
    Gestisce la complessitÃ  di:
    - Scaricare modelli da HuggingFace
    - Applicare quantizzazione 4-bit
    - Configurare LoRA per fine-tuning efficiente
    """
    
    def load_model(self, enable_gradient_checkpointing: bool = True):
        # 1. Configura bitsandbytes per quantizzazione
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",           # Normal Float 4-bit
            bnb_4bit_use_double_quant=True,       # Quantizza anche i parametri
        )
        
        # 2. Carica il modello quantizzato
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            quantization_config=bnb_config,
            device_map="auto",  # Distribuisce automaticamente su GPU
        )
        
        # 3. Prepara per training k-bit (congela layers base)
        model = prepare_model_for_kbit_training(model)
        
        # 4. Applica LoRA (aggiunge adattatori trainable)
        lora_config = LoraConfig(
            r=16,                    # Rank della decomposizione
            lora_alpha=32,           # Scaling factor
            target_modules=[         # Quali layer modificare
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
            ],
            lora_dropout=0.1,
        )
        model = get_peft_model(model, lora_config)
        
        # Ora solo ~1% dei parametri Ã¨ trainable!
        model.print_trainable_parameters()
        # Output: "trainable params: 13M || all params: 7B || 0.18%"
        
        return model
```

### RewardFunction: Valutazione Automatica

```python
# src/agent/agent_lightning_trainer.py

class RewardFunction:
    """
    Il "giudice" che valuta le generazioni del modello.
    
    Senza reward function, il modello non sa cosa migliorare.
    Con reward function, impara a generare risposte migliori.
    """
    
    @staticmethod
    def coding_reward(prompt: str, generation: str) -> float:
        """
        Valuta la qualitÃ  del codice generato.
        
        Criteri:
        - Sintassi corretta (parsabile)
        - Presenza di docstring
        - Type hints
        - Lunghezza appropriata
        """
        reward = 0.0
        
        # Estrai codice dalla risposta
        code_blocks = re.findall(r'```python\n?(.*?)```', generation, re.DOTALL)
        if not code_blocks:
            return -0.5  # Penalizza assenza di codice
        
        code = code_blocks[0]
        
        # Verifica sintassi
        try:
            compile(code, '<string>', 'exec')
            reward += 0.3  # +0.3 per sintassi corretta
        except SyntaxError:
            reward -= 0.3  # -0.3 per errori
        
        # Bonus per best practices
        if '"""' in code:           reward += 0.1  # Docstring
        if ': ' in code and '->':   reward += 0.1  # Type hints
        if 50 < len(code) < 2000:   reward += 0.1  # Lunghezza ragionevole
        
        return max(-1.0, min(1.0, reward))
    
    @staticmethod
    def combined_reward(prompt: str, generation: str) -> float:
        """
        Auto-detect del task type e applica reward appropriato.
        
        Il modello impara a essere bravo in tutto!
        """
        prompt_lower = prompt.lower()
        
        if any(kw in prompt_lower for kw in ['function', 'tool', 'api']):
            return RewardFunction.function_calling_reward(...)
        elif any(kw in prompt_lower for kw in ['code', 'python', 'write']):
            return RewardFunction.coding_reward(...)
        else:
            return RewardFunction.chat_reward(...)
```

### SmartChunker: Chunking Semantico

```python
# src/memory/smart_chunker.py

class SmartChunker:
    """
    Chunker che capisce la struttura del codice.
    
    A differenza del chunking per caratteri, questo:
    - Preserva funzioni complete
    - Mantiene classi con i loro metodi
    - Include contesto per gli embedding
    """
    
    def chunk_python_code(self, code: str, file_path: str):
        # Usa tree-sitter per parsing AST
        parser = self._get_parser("python")
        tree = parser.parse(code.encode())
        
        chunks = []
        
        def process_node(node, parent_class=None):
            if node.type == "function_definition":
                # Estrai la funzione intera
                chunk = CodeChunk(
                    content=self._get_node_text(node),
                    chunk_type=ChunkType.METHOD if parent_class else ChunkType.FUNCTION,
                    name=self._get_node_name(node),
                    docstring=self._extract_docstring(node),
                    parent=parent_class,
                )
                chunks.append(chunk)
                
            elif node.type == "class_definition":
                # Per classi grandi, estrai i metodi separatamente
                class_name = self._get_node_name(node)
                for child in node.children:
                    process_node(child, parent_class=class_name)
        
        # Processa l'AST
        process_node(tree.root_node)
        
        return chunks
    
    def to_embedding_text(self, chunk: CodeChunk) -> str:
        """
        Genera testo ottimizzato per embedding.
        
        Aggiunge contesto per migliorare la ricerca semantica.
        """
        parts = []
        
        # Header con metadata
        if chunk.chunk_type == ChunkType.FUNCTION:
            parts.append(f"# Function: {chunk.name}")
        elif chunk.chunk_type == ChunkType.METHOD:
            parts.append(f"# Method: {chunk.parent}.{chunk.name}")
        
        # Docstring come descrizione
        if chunk.docstring:
            parts.append(f"# Description: {chunk.docstring[:200]}")
        
        # Il codice vero e proprio
        parts.append(chunk.content)
        
        return "\n".join(parts)
```

---

## ğŸ“š Citazioni e Riferimenti

Questo progetto si basa su ricerca e strumenti open source. Ecco i contributi che hanno reso tutto possibile:

### ğŸ“„ Paper Accademici

| Paper | Autori | Contributo |
|-------|--------|------------|
| **[LUFFY](https://arxiv.org/abs/2504.14945)** ğŸ†• | Yan et al. (2025) | Off-Policy Reasoning Learning (NeurIPS 2025) |
| **[DeepSeek-R1](https://arxiv.org/abs/2501.12948)** ğŸ†• | DeepSeek (2025) | Reinforcement Learning per Reasoning |
| **[ExGRPO](https://arxiv.org/abs/2510.02245)** ğŸ†• | Zhan et al. (2025) | Learning from Model's Own Experience |
| **[QLoRA](https://arxiv.org/abs/2305.14314)** | Dettmers et al. (2023) | Quantizzazione 4-bit per fine-tuning efficiente |
| **[LoRA](https://arxiv.org/abs/2106.09685)** | Hu et al. (2021) | Low-Rank Adaptation per PEFT |
| **[GRPO](https://arxiv.org/abs/2402.03300)** | Shao et al. (2024) | Group Relative Policy Optimization |
| **[ColBERT](https://arxiv.org/abs/2004.12832)** | Khattab & Zaharia (2020) | Late interaction per reranking |

### ğŸ› ï¸ Librerie e Framework

| Progetto | Licenza | Uso in questo progetto |
|----------|---------|------------------------|
| [LUFFY](https://github.com/ElliottYan/LUFFY) ğŸ†• | MIT | Off-Policy Reasoning Learning |
| [Search-R1](https://github.com/PeterGriffinJin/Search-R1) ğŸ†• | MIT | Reasoning with Search Integration |
| [veRL](https://github.com/volcengine/verl) ğŸ†• | Apache 2.0 | Scalable RL Training |
| [vLLM](https://github.com/vllm-project/vllm) ğŸ†• | Apache 2.0 | Fast Inference for RL |
| [Microsoft Agent Lightning](https://github.com/microsoft/agent-lightning) | MIT | Training RL per agenti AI |
| [HuggingFace Transformers](https://github.com/huggingface/transformers) | Apache 2.0 | Modelli e tokenizer |
| [PyTorch Lightning](https://github.com/Lightning-AI/lightning) | Apache 2.0 | Orchestrazione training |
| [PEFT](https://github.com/huggingface/peft) | Apache 2.0 | LoRA e altri adapter |
| [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | MIT | Quantizzazione 4-bit |
| [ChromaDB](https://github.com/chroma-core/chroma) | Apache 2.0 | Vector database per RAG |
| [FAISS](https://github.com/facebookresearch/faiss) ğŸ†• | MIT | Vector similarity search |
| [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) | Apache 2.0 | Embedding e reranking |
| [tree-sitter](https://github.com/tree-sitter/tree-sitter) | MIT | Parsing AST per chunking |

### ğŸ’¡ Ispirazione

- **[LUFFY](https://github.com/ElliottYan/LUFFY)** ğŸ†• - Off-policy learning per reasoning models
- **[Search-R1](https://github.com/PeterGriffinJin/Search-R1)** ğŸ†• - Reasoning con ricerca integrata
- **[DeepSeek-R1](https://api-docs.deepseek.com/)** ğŸ†• - Reasoning traces per training
- **[osgrep](https://github.com/Ryandonofrio3/osgrep)** - Ispirazione per smart chunking e reranking
- **[LlamaIndex](https://github.com/run-llama/llama_index)** - Pattern architetturali per RAG
- **[LangChain](https://github.com/langchain-ai/langchain)** - Integrazioni document loaders

---

## ğŸ“Š Benchmark e Risultati

### Memory Usage (Mistral 7B)

| Configurazione | VRAM | Trainable Params |
|----------------|------|------------------|
| Full Fine-tuning (FP32) | ~28GB | 7B (100%) |
| Full Fine-tuning (FP16) | ~14GB | 7B (100%) |
| **QLoRA 4-bit + LoRA** | **~6GB** | **13M (0.18%)** |

### Training Speed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TEMPO PER 1000 STEP (Mistral 7B)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Full FP32:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~4 ore        â”‚
â”‚  Full FP16:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~2 ore                      â”‚
â”‚  QLoRA + LoRA:  â–ˆâ–ˆâ–ˆâ–ˆ ~30 min                               â”‚
â”‚                                                             â”‚
â”‚  (RTX 4090, batch_size=2, gradient_accumulation=8)         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

```bash
# 1. Clona il progetto
git clone https://github.com/tuousername/llm-finetuning-agent-lightning.git
cd llm-finetuning-agent-lightning

# 2. Installa dipendenze
pip install -e .

# 3. Training classico (PyTorch Lightning)
python main.py --config config/config.yaml

# 4. Training RL con Agent Lightning
python main_agent_lightning.py --config config/config.yaml

# 5. Training con LUFFY (Off-Policy Reasoning) ğŸ†•
python main_reasoning.py --mode luffy --config config/config.yaml

# 6. Training con Search-R1 (Reasoning + Search) ğŸ†•
python main_reasoning.py --mode search-r1 --config config/config.yaml --kb ./data/knowledge_base

# 7. Training combinato LUFFY + Search-R1 ğŸ†•
python main_reasoning.py --mode combined --config config/config.yaml
```

### Opzioni avanzate per reasoning:

```bash
# Carica tracce off-policy da DeepSeek-R1
python main_reasoning.py --mode luffy --traces ./data/deepseek_r1_traces.json

# Verifica configurazione senza training
python main_reasoning.py --mode combined --dry-run
```

---

## ğŸ“ Struttura del Progetto

```
.
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Configurazione principale (include LUFFY/Search-R1)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_loader.py      # Caricamento modelli con QLoRA
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_module.py       # DataModule + Multi-Source Training
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ training_agent.py    # LightningModule per training
â”‚   â”‚   â”œâ”€â”€ agent_lightning_trainer.py  # Agent Lightning RL
â”‚   â”‚   â””â”€â”€ tools.py             # Definizioni tool per agenti
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # VectorStore (ChromaDB + Reranker)
â”‚   â”‚   â”œâ”€â”€ procedural_memory.py # SOP Manager
â”‚   â”‚   â””â”€â”€ smart_chunker.py     # Smart chunking con tree-sitter
â”‚   â””â”€â”€ reasoning/               # ğŸ†• Modulo LUFFY + Search-R1
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ luffy_trainer.py     # LUFFY: Off-Policy Reasoning Learning
â”‚       â””â”€â”€ search_r1.py         # Search-R1: Reasoning with Search
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sops/                    # SOP personalizzate
â”‚   â””â”€â”€ reasoning_traces/        # ğŸ†• Tracce off-policy (DeepSeek-R1)
â”œâ”€â”€ main.py                      # Entry point (PyTorch Lightning)
â”œâ”€â”€ main_agent_lightning.py      # Entry point (Agent Lightning RL)
â”œâ”€â”€ main_reasoning.py            # ğŸ†• Entry point (LUFFY + Search-R1)
â””â”€â”€ README.md
```

---

## ğŸ‘¤ Autore

**[Alessandro Boni]**

- ğŸŒ Portfolio: [tuo-sito.com](https://alessandroboni.netlify.app/)
- ğŸ’¼ LinkedIn: [linkedin.com/in/tuoprofilo](https://www.linkedin.com/in/alessandro-boni-503129172/)
- ğŸ™ GitHub: [@SandroHub013](https://github.com/SandroHub013)

---

## ğŸ“„ Licenza

Questo progetto Ã¨ rilasciato sotto licenza **MIT**.

```
MIT License

Copyright (c) 2024 [Il Tuo Nome]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

<p align="center">
  <sub>Built with â¤ï¸ and lots of â˜• for the AI community</sub>
</p>



