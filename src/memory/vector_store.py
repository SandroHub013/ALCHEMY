"""
Vector Store for Retrieval Augmented Generation (RAG).

Implements a simple wrapper around ChromaDB with local embeddings
generated by sentence-transformers.

Features:
- Local embeddings with sentence-transformers
- Optional reranking with CrossEncoder (inspired by osgrep)
- Smart chunking for Python code with tree-sitter

Usage:
    ```python
    from src.memory import VectorStore
    
    # Initialize (with optional reranking)
    store = VectorStore(use_reranker=True)
    
    # Add documents
    store.add_documents([
        "Python is a programming language.",
        "Machine Learning uses algorithms to learn from data.",
    ])
    
    # Query (automatic reranking if enabled)
    results = store.query("What is Python?", n_results=3)
    for doc, score, metadata in results:
        print(f"Score: {score:.3f} - {doc}")
    ```
"""

from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass, field
import logging
import os
import hashlib

logger = logging.getLogger(__name__)

# Lazy imports to avoid errors if not installed
_chromadb = None
_sentence_transformers = None
_cross_encoder = None


def _get_chromadb():
    """Lazy import of ChromaDB."""
    global _chromadb
    if _chromadb is None:
        try:
            import chromadb
            _chromadb = chromadb
        except ImportError:
            raise ImportError(
                "ChromaDB not installed. Install with: pip install chromadb"
            )
    return _chromadb


def _get_sentence_transformers():
    """Lazy import of sentence-transformers."""
    global _sentence_transformers
    if _sentence_transformers is None:
        try:
            from sentence_transformers import SentenceTransformer
            _sentence_transformers = SentenceTransformer
        except ImportError:
            raise ImportError(
                "sentence-transformers not installed. "
                "Install with: pip install sentence-transformers"
            )
    return _sentence_transformers


def _get_cross_encoder():
    """Lazy import of CrossEncoder for reranking."""
    global _cross_encoder
    if _cross_encoder is None:
        try:
            from sentence_transformers import CrossEncoder
            _cross_encoder = CrossEncoder
        except ImportError:
            raise ImportError(
                "sentence-transformers not installed or version too old. "
                "Install/upgrade with: pip install -U sentence-transformers"
            )
    return _cross_encoder


@dataclass
class Document:
    """Represents a document in the vector store."""
    
    id: str
    text: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None
    
    @classmethod
    def from_text(cls, text: str, metadata: Optional[Dict[str, Any]] = None) -> "Document":
        """Create a document from text, generating a unique ID."""
        doc_id = hashlib.md5(text.encode()).hexdigest()[:12]
        return cls(id=doc_id, text=text, metadata=metadata or {})


class Reranker:
    """
    Reranker based on CrossEncoder to improve search results.
    
    Inspired by osgrep which uses ColBERT for reranking.
    CrossEncoder evaluates the relevance of each (query, document) pair
    directly, producing more accurate scores than simple cosine similarity.
    
    Attributes:
        model_name: Name of the CrossEncoder model
        
    Recommended models (from fastest to most accurate):
        - "cross-encoder/ms-marco-MiniLM-L-6-v2": Fast, good quality
        - "cross-encoder/ms-marco-MiniLM-L-12-v2": Balanced
        - "BAAI/bge-reranker-base": High quality, multilingual
    """
    
    RERANKER_MODELS = {
        "fast": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "balanced": "cross-encoder/ms-marco-MiniLM-L-12-v2",
        "accurate": "BAAI/bge-reranker-base",
    }
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """
        Initialize the Reranker.
        
        Args:
            model_name: Model name or shortcut ("fast", "balanced", "accurate")
        """
        # Resolve shortcut
        self.model_name = self.RERANKER_MODELS.get(model_name, model_name)
        
        logger.info(f"Loading reranker: {self.model_name}")
        CrossEncoder = _get_cross_encoder()
        self.model = CrossEncoder(self.model_name)
        logger.info("Reranker loaded")
    
    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: Optional[int] = None,
    ) -> List[Tuple[int, float]]:
        """
        Reorder documents by relevance to the query.
        
        Args:
            query: Search query
            documents: List of documents to reorder
            top_k: Number of results to return (None = all)
            
        Returns:
            List of tuples (original_index, score) sorted by descending score
        """
        if not documents:
            return []
        
        # Create query-document pairs
        pairs = [(query, doc) for doc in documents]
        
        # Calculate scores
        scores = self.model.predict(pairs)
        
        # Create list (index, score) and sort
        indexed_scores = list(enumerate(scores))
        indexed_scores.sort(key=lambda x: -x[1])
        
        if top_k is not None:
            indexed_scores = indexed_scores[:top_k]
        
        return indexed_scores
    
    def rerank_with_docs(
        self,
        query: str,
        documents: List[Tuple[str, float, Dict[str, Any]]],
        top_k: Optional[int] = None,
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        Reorder query results (with metadata) by relevance.
        
        Args:
            query: Search query
            documents: List of tuples (document, original_score, metadata)
            top_k: Number of results to return
            
        Returns:
            Reordered list of tuples (document, new_score, metadata)
        """
        if not documents:
            return []
        
        # Extract only texts
        texts = [doc for doc, _, _ in documents]
        
        # Rerank
        reranked = self.rerank(query, texts, top_k)
        
        # Reconstruct with new scores
        result = []
        for original_idx, new_score in reranked:
            doc, _, metadata = documents[original_idx]
            result.append((doc, float(new_score), metadata))
        
        return result


class VectorStore:
    """
    Vector Store for RAG based on ChromaDB.
    
    Uses sentence-transformers to generate embeddings locally,
    without dependencies on external APIs.
    
    Features:
        - Local embeddings with sentence-transformers
        - Optional reranking with CrossEncoder for more accurate results
        - Support for smart chunking of Python code
    
    Attributes:
        collection_name: Name of the ChromaDB collection
        embedding_model: Name of the sentence-transformers model
        persist_directory: Directory for persistence (None = in-memory)
        use_reranker: Whether to enable result reranking
        reranker_model: Model for reranking
    """
    
    # Recommended models (from fastest to most accurate)
    EMBEDDING_MODELS = {
        "fast": "all-MiniLM-L6-v2",           # 384 dim, fast
        "balanced": "all-mpnet-base-v2",       # 768 dim, balanced
        "multilingual": "paraphrase-multilingual-MiniLM-L12-v2",  # Multilingual
        "italian": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    }
    
    def __init__(
        self,
        collection_name: str = "knowledge_base",
        embedding_model: str = "all-MiniLM-L6-v2",
        persist_directory: Optional[str] = None,
        use_reranker: bool = False,
        reranker_model: str = "fast",
    ):
        """
        Initialize the VectorStore.
        
        Args:
            collection_name: Name of the collection
            embedding_model: Model for embeddings (default: all-MiniLM-L6-v2)
            persist_directory: Directory to save data (None = RAM only)
            use_reranker: Whether to enable reranking with CrossEncoder
            reranker_model: Reranker model ("fast", "balanced", "accurate" or full name)
        """
        self.collection_name = collection_name
        self.embedding_model_name = embedding_model
        self.persist_directory = persist_directory
        self.use_reranker = use_reranker
        
        # Initialize embedding model
        logger.info(f"Loading embedding model: {embedding_model}")
        SentenceTransformer = _get_sentence_transformers()
        self.embedding_model = SentenceTransformer(embedding_model)
        self._embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        logger.info(f"Embedding dimension: {self._embedding_dim}")
        
        # Initialize reranker if requested
        self.reranker: Optional[Reranker] = None
        if use_reranker:
            self.reranker = Reranker(reranker_model)
        
        # Initialize ChromaDB
        chromadb = _get_chromadb()
        if persist_directory:
            os.makedirs(persist_directory, exist_ok=True)
            self.client = chromadb.PersistentClient(path=persist_directory)
            logger.info(f"ChromaDB persistent in: {persist_directory}")
        else:
            self.client = chromadb.Client()
            logger.info("ChromaDB in-memory (data lost on restart)")
        
        # Create/get collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Use cosine similarity
        )
        
        logger.info(
            f"VectorStore initialized - Collection: {collection_name}, "
            f"Documents: {self.collection.count()}, Reranker: {use_reranker}"
        )
    
    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts
            
        Returns:
            List of embeddings (each embedding is a list of floats)
        """
        embeddings = self.embedding_model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=len(texts) > 10,
        )
        return embeddings.tolist()
    
    def add_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """
        Add documents to the vector store.
        
        Args:
            texts: List of texts to add
            metadatas: Optional metadata for each document
            ids: Optional IDs (automatically generated if not provided)
            
        Returns:
            List of IDs of added documents
        """
        if not texts:
            return []
        
        # Generate IDs if not provided
        if ids is None:
            ids = [
                hashlib.md5(text.encode()).hexdigest()[:12]
                for text in texts
            ]
        
        # Prepare metadata
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        # Generate embeddings
        logger.info(f"Generating embeddings for {len(texts)} documents...")
        embeddings = self._generate_embeddings(texts)
        
        # Add to ChromaDB
        self.collection.add(
            ids=ids,
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
        )
        
        logger.info(f"Added {len(texts)} documents. Total: {self.collection.count()}")
        return ids
    
    def add_document(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
        doc_id: Optional[str] = None,
    ) -> str:
        """
        Add a single document.
        
        Args:
            text: Document text
            metadata: Optional metadata
            doc_id: Optional ID
            
        Returns:
            ID of the added document
        """
        ids = self.add_documents(
            texts=[text],
            metadatas=[metadata] if metadata else None,
            ids=[doc_id] if doc_id else None,
        )
        return ids[0]
    
    def query(
        self,
        text: str,
        n_results: int = 3,
        where: Optional[Dict[str, Any]] = None,
        use_reranker: Optional[bool] = None,
        rerank_top_k: Optional[int] = None,
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        Search for documents most similar to a query.
        
        If the reranker is enabled, retrieves more initial results and then
        reorders them by relevance using CrossEncoder.
        
        Args:
            text: Query text
            n_results: Number of results to return
            where: Optional filter on metadata
            use_reranker: Override to use/not use reranker (None = use default)
            rerank_top_k: How many initial results to retrieve for reranking
                         (default: n_results * 3)
            
        Returns:
            List of tuples (document, score, metadata)
            Higher score = more relevant
        """
        # Determine whether to use reranker
        should_rerank = use_reranker if use_reranker is not None else self.use_reranker
        should_rerank = should_rerank and self.reranker is not None
        
        # If reranking, retrieve more initial results
        if should_rerank:
            initial_n = rerank_top_k or (n_results * 3)
        else:
            initial_n = n_results
        
        # Generate query embedding
        query_embedding = self._generate_embeddings([text])[0]
        
        # Query ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=initial_n,
            where=where,
            include=["documents", "distances", "metadatas"],
        )
        
        # Format results
        # ChromaDB returns distances, convert to similarity (1 - distance for cosine)
        output = []
        if results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                distance = results["distances"][0][i]
                # Cosine distance -> similarity
                similarity = 1 - distance
                metadata = results["metadatas"][0][i] if results["metadatas"] else {}
                output.append((doc, similarity, metadata))
        
        # Apply reranking if enabled
        if should_rerank and output and self.reranker:
            output = self.reranker.rerank_with_docs(text, output, top_k=n_results)
        
        return output[:n_results]
    
    def query_with_context(
        self,
        text: str,
        n_results: int = 3,
        separator: str = "\n\n---\n\n",
    ) -> str:
        """
        Search documents and format them as context for RAG.
        
        Args:
            text: Query
            n_results: Number of results
            separator: Separator between documents
            
        Returns:
            Formatted string with retrieved documents
        """
        results = self.query(text, n_results=n_results)
        
        if not results:
            return "No relevant context found."
        
        context_parts = []
        for i, (doc, score, metadata) in enumerate(results, 1):
            source = metadata.get("source", "unknown")
            context_parts.append(f"[{i}] (score: {score:.2f}, source: {source})\n{doc}")
        
        return separator.join(context_parts)
    
    def delete(self, ids: List[str]) -> None:
        """Delete documents by ID."""
        self.collection.delete(ids=ids)
        logger.info(f"Deleted {len(ids)} documents")
    
    def clear(self) -> None:
        """Delete all documents from the collection."""
        # ChromaDB doesn't have a clear method, recreate the collection
        self.client.delete_collection(self.collection_name)
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        logger.info("Collection cleared")
    
    def count(self) -> int:
        """Return the number of documents."""
        return self.collection.count()
    
    def get_all(self) -> List[Tuple[str, str, Dict[str, Any]]]:
        """
        Return all documents.
        
        Returns:
            List of tuples (id, document, metadata)
        """
        results = self.collection.get(include=["documents", "metadatas"])
        
        output = []
        if results["ids"]:
            for i, doc_id in enumerate(results["ids"]):
                doc = results["documents"][i] if results["documents"] else ""
                metadata = results["metadatas"][i] if results["metadatas"] else {}
                output.append((doc_id, doc, metadata))
        
        return output
    
    def add_code_chunks(
        self,
        chunks: List[Any],  # List[CodeChunk] - avoid circular import
    ) -> List[str]:
        """
        Add code chunks from SmartChunker.
        
        Args:
            chunks: List of CodeChunk from the smart_chunker module
            
        Returns:
            List of IDs of added documents
        """
        if not chunks:
            return []
        
        texts = []
        metadatas = []
        
        for chunk in chunks:
            # Use to_embedding_text if available, otherwise content
            if hasattr(chunk, "to_embedding_text"):
                texts.append(chunk.to_embedding_text())
            else:
                texts.append(str(chunk.content))
            
            # Build metadata
            metadata = {
                "source": getattr(chunk, "file_path", ""),
                "chunk_type": getattr(chunk, "chunk_type", "unknown"),
                "name": getattr(chunk, "qualified_name", getattr(chunk, "name", "")),
                "start_line": getattr(chunk, "start_line", 0),
                "end_line": getattr(chunk, "end_line", 0),
            }
            
            # Convert enum if necessary
            if hasattr(metadata["chunk_type"], "value"):
                metadata["chunk_type"] = metadata["chunk_type"].value
            
            metadatas.append(metadata)
        
        return self.add_documents(texts, metadatas)


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def create_vector_store(
    persist_path: Optional[str] = "./vector_store",
    model: str = "fast",
    use_reranker: bool = False,
    reranker_model: str = "fast",
) -> VectorStore:
    """
    Factory function to create a VectorStore with common configuration.
    
    Args:
        persist_path: Path for persistence (None = in-memory)
        model: "fast", "balanced", "multilingual", or full model name
        use_reranker: Whether to enable reranking for more accurate results
        reranker_model: "fast", "balanced", "accurate" or full model name
        
    Returns:
        Configured VectorStore
        
    Example:
        ```python
        # Basic VectorStore
        store = create_vector_store()
        
        # With reranking for better results
        store = create_vector_store(use_reranker=True)
        
        # Multilingual with accurate reranker
        store = create_vector_store(
            model="multilingual",
            use_reranker=True,
            reranker_model="accurate",
        )
        ```
    """
    # Resolve model shortcut
    embedding_model = VectorStore.EMBEDDING_MODELS.get(model, model)
    
    return VectorStore(
        collection_name="knowledge_base",
        embedding_model=embedding_model,
        persist_directory=persist_path,
        use_reranker=use_reranker,
        reranker_model=reranker_model,
    )


def load_documents_from_file(
    file_path: str,
    chunk_size: int = 500,
    chunk_overlap: int = 50,
) -> List[str]:
    """
    Load and chunk a text file.
    
    Args:
        file_path: Path to the file
        chunk_size: Maximum chunk size (characters)
        chunk_overlap: Overlap between chunks
        
    Returns:
        List of text chunks
    """
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
    
    # Simple chunking by paragraphs/sentences
    chunks = []
    paragraphs = text.split("\n\n")
    
    current_chunk = ""
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        if len(current_chunk) + len(para) < chunk_size:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\n\n"
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
