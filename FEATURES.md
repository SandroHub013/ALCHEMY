# ‚ú® Features Tecniche

Panoramica completa delle funzionalit√† implementate in questo framework.

---

## üéØ Overview

| Categoria | Feature | Status |
|-----------|---------|--------|
| **Training** | QLoRA 4-bit | ‚úÖ Completo |
| | PEFT/LoRA | ‚úÖ Completo |
| | Multi-Source Training | ‚úÖ Completo |
| | Gradient Checkpointing | ‚úÖ Completo |
| **RL** | Agent Lightning Integration | ‚úÖ Completo |
| | GRPO Algorithm | ‚úÖ Completo |
| | Custom Reward Functions | ‚úÖ Completo |
| | APO (Prompt Optimization) | ‚úÖ Completo |
| **Reasoning** üÜï | LUFFY Off-Policy Learning | ‚úÖ Completo |
| | ExGRPO (Self-Experience) | ‚úÖ Completo |
| | DeepSeek-R1 Integration | ‚úÖ Completo |
| | Search-R1 (Reasoning + Search) | ‚úÖ Completo |
| **Memory** | VectorStore (ChromaDB) | ‚úÖ Completo |
| | CrossEncoder Reranking | ‚úÖ Completo |
| | Smart Chunking | ‚úÖ Completo |
| | SOP (Procedural Memory) | ‚úÖ Completo |
| | FAISS Vector Search üÜï | ‚úÖ Completo |
| | Hybrid Search (Vector + BM25) üÜï | ‚úÖ Completo |

---

## ü¶ä LUFFY - Off-Policy Reasoning Learning (NEW)

### Overview

**[LUFFY](https://arxiv.org/abs/2504.14945)** (Learning to Reason under Off-Policy Guidance) √® un framework per migliorare le capacit√† di ragionamento dei modelli usando tracce off-policy da modelli avanzati come DeepSeek-R1.

**Accettato a NeurIPS 2025!**

### Modalit√† Supportate

| Modalit√† | Descrizione | Quando Usarlo |
|----------|-------------|---------------|
| **luffy** | Usa tracce da modelli esterni (DeepSeek-R1) | Quando hai accesso a reasoning traces di alta qualit√† |
| **exgrpo** | Impara dall'esperienza propria | Quando non hai tracce esterne |
| **hybrid** | Combina entrambi | Per massimizzare le capacit√† |

### Implementazione

```python
# src/reasoning/luffy_trainer.py

from src.reasoning import LuffyTrainer, LuffyConfig, OffPolicyMode

config = LuffyConfig(
    mode=OffPolicyMode.LUFFY,
    off_policy_source="deepseek-r1",
    off_policy_weight=0.5,
    on_policy_weight=0.5,
    kl_coef=0.05,
)

trainer = LuffyTrainer(model, tokenizer, config)

# Carica tracce off-policy
trainer.load_off_policy_traces("deepseek_r1_traces.json")

# Training
results = trainer.train(train_dataset, num_epochs=3)
```

### Off-Policy Mixer

```python
class OffPolicyDataMixer:
    """Combina dati on-policy e off-policy con prioritized sampling."""
    
    def sample_mixed_batch(self, on_policy_data, batch_size):
        # Calcola split
        off_policy_count = int(batch_size * self.config.off_policy_weight)
        on_policy_count = batch_size - off_policy_count
        
        # Campiona off-policy con filtering
        off_policy = self._sample_filtered(
            min_reward=self.config.min_off_policy_reward
        )
        
        # Combina e shuffle
        return mixed_batch
```

### ExGRPO (Learning from Own Experience)

```python
# ExGRPO: Impara dall'esperienza del modello stesso
config = LuffyConfig(
    mode=OffPolicyMode.EXGRPO,
    experience_buffer_size=10000,
    experience_sample_ratio=0.3,
)

# Il trainer salva automaticamente esperienze positive
# e le riutilizza nel training successivo
```

### Benchmark Results

| Model | AIME 2024 | AIME 2025 | MATH-500 | Avg |
|-------|-----------|-----------|----------|-----|
| Qwen2.5-Math-7B (baseline) | 11.5 | 4.9 | 43.6 | 19.0 |
| + SFT Only | 22.2 | 22.3 | 82.6 | 44.1 |
| + SFT + RL | 25.8 | 23.1 | 87.2 | 48.2 |
| **+ LUFFY** | **29.4** | **23.1** | **87.6** | **50.1** |

---

## üîç Search-R1 - Reasoning with Search (NEW)

### Overview

**[Search-R1](https://github.com/PeterGriffinJin/Search-R1)** permette al modello di cercare informazioni durante il ragionamento, combinando retrieval e reasoning in modo fluido.

### Componenti Principali

| Componente | Descrizione |
|------------|-------------|
| **SearchEngine** | Interfaccia per diversi tipi di ricerca |
| **ReasoningWithSearch** | Orchestratore che integra search nel reasoning |
| **SearchR1Trainer** | Trainer RL per ottimizzare la policy di search |

### Search Engine Types

```python
from src.reasoning import create_search_engine

# Vector Search (semantic similarity)
vector_engine = create_search_engine("vector", documents=docs)

# BM25 Search (keyword-based)
bm25_engine = create_search_engine("bm25", documents=docs)

# Hybrid Search (vector + BM25 con RRF fusion)
hybrid_engine = create_search_engine(
    "hybrid",
    documents=docs,
    vector_weight=0.5,
    bm25_weight=0.5
)
```

### Reasoning Flow

```python
# src/reasoning/search_r1.py

class ReasoningWithSearch:
    def reason(self, question: str) -> Dict[str, Any]:
        # 1. Genera pensiero iniziale
        # 2. Rileva <search>query</search>
        # 3. Esegue ricerca
        # 4. Inietta <context>risultati</context>
        # 5. Continua ragionamento
        # 6. Estrai risposta finale
        
        return {
            "question": question,
            "final_answer": answer,
            "reasoning_trace": trace,
            "search_queries": queries,
        }
```

### Special Tokens

| Token | Descrizione |
|-------|-------------|
| `<search>` | Inizia query di ricerca |
| `</search>` | Fine query |
| `<context>` | Inizio risultati |
| `</context>` | Fine risultati |

### Esempio Completo

```python
from src.reasoning import (
    SearchR1Trainer, 
    SearchR1Config,
    HybridSearchEngine,
)

# Configura search engine
search_engine = HybridSearchEngine(
    documents=knowledge_base,
    vector_weight=0.6,
    bm25_weight=0.4,
)

# Configura trainer
config = SearchR1Config(
    max_search_calls=3,
    context_window=3,
    use_cot=True,
    reward_search_bonus=0.1,
)

trainer = SearchR1Trainer(
    model=model,
    tokenizer=tokenizer,
    search_engine=search_engine,
    config=config,
)

# Training
results = trainer.train(
    train_data=[{"question": "...", "answer": "..."}, ...],
    num_epochs=3,
)
```

### Reward Function

```python
def search_reward(question, answer, reference, search_used):
    reward = 0.0
    
    # Correttezza risposta (peso: 0.7)
    if reference.lower() in answer.lower():
        reward += 0.5
    
    # Bonus per uso efficace della ricerca (peso: 0.1)
    if search_used:
        reward += 0.1
    
    # Qualit√† ragionamento (peso: 0.2)
    if "therefore" in answer.lower() or "step" in answer.lower():
        reward += 0.2
    
    return reward
```

---

## üî¨ Training Efficiente

### QLoRA 4-bit Quantization

**Problema risolto:** I modelli LLM richiedono troppa memoria per GPU consumer.

**Soluzione implementata:**

```python
# src/models/model_loader.py

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # Carica in 4-bit
    bnb_4bit_compute_dtype=torch.float16, # Compute in FP16
    bnb_4bit_quant_type="nf4",           # Normal Float 4-bit
    bnb_4bit_use_double_quant=True,       # Quantizza i quantization params
)
```

**Risultati:**
- Mistral 7B: da 28GB ‚Üí 6GB VRAM
- Riduzione 78% memoria
- Performance mantenuta

### LoRA (Low-Rank Adaptation)

**Problema risolto:** Aggiornare tutti i parametri √® costoso e rischia overfitting.

**Implementazione:**

```python
lora_config = LoraConfig(
    r=16,                # Rank della decomposizione
    lora_alpha=32,       # Scaling factor
    target_modules=[     # Solo layer critici
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_dropout=0.1,
    task_type="CAUSAL_LM",
)
```

**Risultati:**
- Parametri trainable: 0.18% (13M su 7B)
- Training 50x pi√π veloce
- Adapter modulari e combinabili

### Multi-Source Training

**Problema risolto:** Training su un solo task causa Catastrophic Forgetting.

**Soluzione:**

```yaml
# config/config.yaml
datasets:
  multi_source_enabled: true
  sources:
    - name: "glaiveai/glaive-function-calling-v2"
      weight: 0.30
      type: "function_calling"
    - name: "nickrosh/Evol-Instruct-Code-80k-v1"
      weight: 0.30
      type: "coding"
    - name: "teknium/OpenHermes-2.5"
      weight: 0.30
      type: "chat"
```

**Caratteristiche:**
- Sampling pesato per bilanciare i task
- Formatter unificato (ChatML)
- Preserva competenze esistenti

---

## ü§ñ Reinforcement Learning

### Agent Lightning Integration

**Framework:** Microsoft Agent Lightning per training RL di agenti AI.

**Algoritmi supportati:**

| Algoritmo | Descrizione | Quando usarlo |
|-----------|-------------|---------------|
| **SFT** | Supervised Fine-Tuning | Training iniziale |
| **GRPO** | Group Relative Policy Optimization | Migliorare comportamento |
| **APO** | Automatic Prompt Optimization | Ottimizzare system prompt |

### GRPO Implementation

```python
# src/agent/agent_lightning_trainer.py

class AgentLightningTrainer:
    def train(self, train_dataset, ...):
        for batch in train_dataset:
            # 1. Genera K risposte diverse
            generations = self.model.generate(
                batch["prompt"],
                num_return_sequences=self.config.num_generations,
                temperature=self.config.temperature,
            )
            
            # 2. Calcola reward per ogni risposta
            rewards = [self.reward_fn(prompt, gen) for gen in generations]
            
            # 3. Normalizza rewards (relative)
            advantages = (rewards - mean(rewards)) / std(rewards)
            
            # 4. Policy gradient con KL penalty
            loss = -log_prob(generations) * advantages + kl_coef * KL_div
            
            # 5. Update
            loss.backward()
            optimizer.step()
```

### Reward Functions

**Coding Reward:**
```python
def coding_reward(prompt, generation):
    reward = 0.0
    
    # Sintassi corretta
    try:
        compile(code, '<string>', 'exec')
        reward += 0.3
    except SyntaxError:
        reward -= 0.3
    
    # Best practices
    if has_docstring(code):    reward += 0.1
    if has_type_hints(code):   reward += 0.1
    if appropriate_length(code): reward += 0.1
    
    return reward
```

**Function Calling Reward:**
```python
def function_calling_reward(prompt, generation):
    # JSON valido
    try:
        fc = json.loads(extract_function_call(generation))
        reward += 0.3
    except:
        return -0.3
    
    # Struttura corretta
    if "name" in fc and "arguments" in fc:
        reward += 0.2
    
    # Tool esistente
    if fc["name"] in available_tools:
        reward += 0.2
    
    return reward
```

**Combined Reward (Auto-detect):**
```python
def combined_reward(prompt, generation):
    # Rileva automaticamente il tipo di task
    if is_coding_task(prompt):
        return coding_reward(prompt, generation)
    elif is_function_calling(prompt):
        return function_calling_reward(prompt, generation)
    else:
        return chat_reward(prompt, generation)
```

---

## üß† Sistema di Memoria

### VectorStore con Reranking

**Architettura a due fasi:**

```
Query ‚Üí Bi-Encoder (recall) ‚Üí Top-K candidati ‚Üí Cross-Encoder (precision) ‚Üí Top-N finali
```

**Implementazione:**

```python
# src/memory/vector_store.py

class VectorStore:
    def __init__(self, use_reranker=True):
        # Fase 1: Bi-Encoder per recall veloce
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        
        # Fase 2: Cross-Encoder per precision
        if use_reranker:
            self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    
    def query(self, text, n_results=3):
        # Fase 1: Recupera candidati (3x n_results)
        candidates = self._vector_search(text, n_results * 3)
        
        # Fase 2: Rerank con Cross-Encoder
        if self.reranker:
            candidates = self.reranker.rerank(text, candidates, top_k=n_results)
        
        return candidates
```

### Smart Chunking

**Problema:** Chunking per caratteri taglia funzioni a met√†.

**Soluzione:** Parsing AST con tree-sitter.

```python
# src/memory/smart_chunker.py

class SmartChunker:
    def chunk_python_code(self, code):
        # Parse AST
        tree = self.parser.parse(code.encode())
        
        chunks = []
        for node in self._traverse(tree.root_node):
            if node.type == "function_definition":
                chunks.append(self._extract_function(node))
            elif node.type == "class_definition":
                chunks.append(self._extract_class(node))
        
        return chunks
```

**Tipi di chunk:**

| Tipo | Descrizione |
|------|-------------|
| `FUNCTION` | Funzione completa con docstring |
| `CLASS` | Classe con header e docstring |
| `METHOD` | Metodo di classe |
| `IMPORT` | Blocco import |
| `DOCSTRING` | Docstring modulo |

**Embedding text ottimizzato:**

```python
def to_embedding_text(chunk):
    """Genera testo con contesto per embedding migliori."""
    return f"""
# {chunk.chunk_type}: {chunk.qualified_name}
# Description: {chunk.docstring[:200]}
# File: {chunk.file_path}

{chunk.content}
"""
```

### SOP (Standard Operating Procedures)

**Scopo:** Guidare il modello attraverso procedure strutturate.

**Struttura SOP:**

```json
{
  "name": "debug_python_code",
  "description": "Procedura per identificare e risolvere bug",
  "trigger": "debug, bug, errore, non funziona",
  "category": "coding",
  "priority": 8,
  "steps": [
    {"action": "Leggi il codice e identifica il problema"},
    {"action": "Classifica l'errore (sintassi, logica, runtime)"},
    {"action": "Localizza la riga problematica"},
    {"action": "Proponi una soluzione con spiegazione"},
    {"action": "Suggerisci test per verificare", "condition": "fix proposto"}
  ]
}
```

**Matching automatico:**

```python
class SOPManager:
    def find_relevant_sop(self, query):
        scores = []
        for sop in self.sops.values():
            score = 0
            for trigger in sop.trigger.split(","):
                if trigger.strip() in query.lower():
                    score += 10
            score += sop.priority * 0.5
            scores.append((sop, score))
        
        return sorted(scores, key=lambda x: -x[1])[0][0]
```

---

## üìä Logging e Monitoraggio

### TensorBoard Integration

```python
# Metriche loggate automaticamente
self.log("train/loss", loss)
self.log("train/perplexity", exp(loss))
self.log("train/reward_mean", reward.mean())
self.log("train/kl_div", kl_divergence)
```

### Agent Lightning Tracing

```python
# Tracciamento dettagliato
with self.tracer.span("generation", {"prompt_len": len(prompt)}):
    output = model.generate(...)
    
agl.emit_generation(
    prompt=prompt,
    response=output,
    model=model_name,
)
```

---

## ‚öôÔ∏è Configurazione

### YAML-based

Tutta la configurazione √® in YAML per flessibilit√†:

```yaml
model:
  name_or_path: "mistralai/Mistral-7B-v0.3"

peft:
  quantization:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
  lora:
    r: 16
    lora_alpha: 32

training:
  num_epochs: 3
  learning_rate: 2.0e-4
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8

agent_lightning:
  enabled: true
  algorithm: "grpo"
  reward_function: "combined"
```

### Override da CLI

```bash
# Override algoritmo
python main_agent_lightning.py --config config.yaml --algorithm grpo

# Dry-run per verificare
python main_agent_lightning.py --config config.yaml --dry-run
```

---

## üîå Estensibilit√†

### Aggiungere un Nuovo Modello

```python
# In model_loader.py
def _get_target_modules_for_model(self, model_name):
    if "my-new-model" in model_name.lower():
        return ["my_attn", "my_mlp"]
    # ... existing models
```

### Aggiungere una Nuova Reward Function

```python
# In agent_lightning_trainer.py
@staticmethod
def my_reward(prompt, generation):
    # Logica custom
    return score

# Registra
RewardFunction.my_reward = my_reward
```

### Aggiungere una Nuova SOP

```json
// data/sops/my_procedure.json
{
  "name": "my_procedure",
  "trigger": "parole, chiave",
  "steps": [...]
}
```

---

*Questa documentazione √® generata per il portfolio tecnico.*

